---
title: "Puttin'a Prior on It: Stan Beta Estmation of Song Lyrics"
author: "MDH"
date: "10/6/2016"
output: html_document
---
<STYLE TYPE="text/css">
<!--
 {
        color: #999999;
        border-width: 1px;
        border-color: #ffffff;
        border-collapse: collapse;
    }
     th {
        border-width: 1px;
        padding: 8px;
        border-style: solid;
        border-color: #ECF0F1;
        background-color: #ECF0F1;
    }
     tr:hover td {
        background-color: #ECF0F1;
        border-color: #ECF0F1;
    }
     td {
        border-width: 1px;
        padding: 8px;
        border-style: solid;
        border-color: #ffffff;
        background-color: #ffffff;
--->
</STYLE>


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Estimating a Beta Distribution with Stan HMC
#### Repository for data, analysis (markdown), and R code


## Introduction
This is a repo to hold the data and code for [my blog post](https://wordpress.com/post/matthewdharris.com/4405) based on Julia Silge's [*Singing the Bayesian Beginner Blues*](http://juliasilge.com/blog/Bayesian-Blues/). The post by Silge is a really fun and interesting analysis of the rate at which song lyrics from *Billboard Hot-100* songs (1958 to present) mention U.S. States by name. In second post on the subject, Silge used a beta distribution to model this rate.  After reading that post, I was inspired to follow up on this model with my current interest in Bayesian modeling with [Stan](http://mc-stan.org/), a probabilistic programming language.  I hoped to repeat Silge's findings while learning how to code her model in Stan.  This was also a fun opportunity to work on more `dplyr` data munging techniques.  The result was a good learning experience and perhaps a few additional insights into the distribution of state name mentions.  

If you are interested in this, please see [my blog post](https://wordpress.com/post/matthewdharris.com/4405)  and [Julia's posts](http://juliasilge.com/blog/Bayesian-Blues/). **Note**: the code and analytical process here is based on [Silge's workflow](https://github.com/juliasilge/juliasilge.github.io/blob/master/_R/2016-09-28-Bayesian-Blues.Rmd), but alterations and additions were made to focus on these different aspects.  Any errors, sloppiness, or misunderstandings are my own.  Also, I am sure there are other way to model these data.  I'd be happy to hear about them, but this post is intended to explore this particular method.  Please contact me if you see errors and just to day hi. [@md_harris])https://twitter.com/Md_Harris

### Differences from Silge's Analysis
1. Is zero inflated to include states not mentioned in lyrics
2. Incorporates mentions of cities with >= 100k population aggregated to their state and compares to analysis without city counts
3. Utilizes Hamiltonian Monte Carlo via `rstan` to estimate parameters, propagated uncertainty, and predict new values.


### Why Go Bayes?
Aside from being an interesting data set due to cultural relevance, it is also a really great example of for why one may want to use a Bayesian approach.  As described in Silge's post, the simple calculation of a mention rate is unsatisfying because it does not consider the magnitude of each states population.  We would prefer a method that incorporates this information and can make a new estimate  based on the number of mentions and population each state that therefore *regularizes* the estimates based on all of the information available. This include prior information and empirical data.  

In my humble opinion this is a really key point; the reason that Silge utilized the beta distribution to *model* the data (as opposed to simply describe) is because she wanted to show the uncertainty for each estimate based on a states mentions and population.  The problem this solves is a state that has a very high rate, but few mentions because it has a low population.  This is in opposition to a state that is mentioned many times, but has a lower rate because of a large population.  This is the classic "batting average" from illuminated by David Robinson [here](http://stats.stackexchange.com/questions/47771/what-is-the-intuition-behind-beta-distribution).

What the Bayesian approach does is incorporate prior knowledge (e.g. priors) and the uncertainty that comes with balancing values based on only a few data points against values based on many data points; which are likely more reliable. By this, the expectation for a states rate of mention is regularized or drawn towards a central global expectation. States with fewer data points are regularized more than more stable states with lots of data.  Finally, the [Credible Interval](https://en.wikipedia.org/wiki/Credible_interval) captures the uncertainty that propagates through this system.  Regularization and uncertainty estimation are key reasons why someone may want to use Bayesian methods.



Let's get started!


#### Packages
```{r libraries, echo=TRUE, message=FALSE, warning=FALSE}
library("ggplot2")
library("grid")
library("dplyr")
library("tools")
library("tidyverse")
library("acs")
library("reshape2")
library("readr")
library("tidytext")
library("ggplot2")
library("ggrepel")
library("broom")
library("fitdistrplus")
library("rstan")
library("knitr")
```


## Get Data
1. Use `acs` package to get state population data from 2014
    i) requires an API from the [Census](http://www.census.gov/developers/)
2. Grab song lyrics from Kaylin Walker's [repo](http://kaylinwalker.com/50-years-of-pop-music/)
3. Load `./data/cities_over_100k_pop.csv`, names of cities with >= 100k population
    i) UNCOMMENT that line `download.file(lyrics_url, save_lyrics_loc)` to run
    ii) you may have to adjust the path
```{r get_data, comment = '', cache=TRUE }
#api.key.install("YOUR KEY HERE!")
## population data
stategeo <- geo.make(state = "*")
popfetch <- acs.fetch(geography = stategeo, 
                      endyear = 2014,
                      span = 5, 
                      table.number = "B01003",
                      col.names = "pretty")
## song lyrics
lyrics_url <- "https://raw.githubusercontent.com/walkerkq/musiclyrics/master/billboard_lyrics_1964-2015.csv"
save_lyrics_loc <- "~/Documents/R_Local/Put_a_prior_on_it-Blog_post/data/billboard_lyrics_1964-2015.csv"
# download.file(lyrics_url, save_lyrics_loc)
song_lyrics <- read_csv(save_lyrics_loc)
cities_dat_loc <- "~/Documents/R_Local/Put_a_prior_on_it-Blog_post/data/cities_over_100k_pop.csv"
```


## Join, mutate, and munge the data
### Prepare data for extracting lyrics
The `tidytext` package uses `unnest_tokens` to do the heavy lifting here.  Thanks [David Robinson](http://varianceexplained.org/) and Julia Silge for this package!
```{r population_data,comment = ''}
# extract desired info from acs data
pop_df <- tbl_df(melt(estimate(popfetch))) %>%
  mutate(name = Var1,
         state_name = tolower(Var1),
         pop2014 = value) %>%
  dplyr::select(name, state_name, pop2014) %>%
  filter(state_name != "puerto rico")

# clean in city names
cities <- read_csv(cities_dat_loc) %>%
  mutate(city = gsub("\x96", "-", city),
         city_name = tolower(city),
         state_name = tolower(state))

# extract and tidy lyrics from songs data
tidy_lyrics <- bind_rows(song_lyrics %>% 
               unnest_tokens(lyric, Lyrics),
               song_lyrics %>% 
               unnest_tokens(lyric, Lyrics, 
               token = "ngrams", n = 2))
```


### Join lyrics to geography
Here the lyrics are joined to the state names to find the songs that contain those names.  An `inner_join` will return only the songs and states that match.  Here I use a `right_join` to retain all 50 state names even if they are not mentioned by any lyrics.  The `zeros` data set is then filtered for a data set that only has matching songs.  Also, I use `distinct(Song, Artist, lyric, ...)` to aggregate the data set to only a single row for each song even if it mentions a song many times. This choice if distinct criteria also allows for songs that mentioned many different states to remain, but collapses an edge case where a song was on the charts in different years.

```{r data_summarize, comment=''}
# join and retain songs whether or not they are in the lyrics
tidy_lyrics_state_zeros <- right_join(tidy_lyrics, pop_df,
                                by = c("lyric" = "state_name")) %>%
  distinct(Song, Artist, lyric, .keep_all = TRUE) %>% 
  mutate(cnt = ifelse(is.na(Source), 0, 1)) %>%
  filter(lyric != "district of columbia") %>%
  dplyr::rename(state_name = lyric)

tidy_lyrics_state <- filter(tidy_lyrics_state_zeros, cnt > 0)

## count the states up
zero_rate <- 0.0000001 # beta hates zeros

# group, summarise, and calculate rate per 100k population
state_counts_zeros <- tidy_lyrics_state_zeros %>% 
  group_by(state_name) %>% 
  dplyr::summarise(n = sum(cnt))  %>% # sum(cnt)
  left_join(pop_df, by = c("state_name" = "state_name")) %>%
  mutate(rate = (n / (pop2014 / 100000)) + zero_rate) %>%
  arrange(desc(n))

# create another data set with 
state_counts <- filter(state_counts_zeros, rate > zero_rate)
print(state_counts)
```


#### Do the same as above, but for states.  
Note the use of `inner_join` here because I am not interested in cities that are not mentioned.
```{r cities, comment=''}
### Cities
## join cities together - inner_join b/c I don't care about cities with zero mentions (right_join otherwise)
tidy_lyrics_city <- inner_join(tidy_lyrics, cities,
                               by = c("lyric" = "city_name")) %>%
  distinct(Song, Artist, lyric, .keep_all = TRUE) %>%
  filter(!city %in% c("Surprise", "Hollywood", 
                      "District of Columbia", "Jackson")) %>%
  mutate(cnt = ifelse(is.na(Source), 0, 1)) %>%
  dplyr::rename(city_name = lyric)

# count cities mentions. No need for a rate; not of use now
city_counts <- tidy_lyrics_city %>% 
  group_by(city_name) %>% 
  dplyr::summarise(n = sum(cnt)) %>%
  arrange(desc(n))
print(city_counts)

# count of states that host the cities mentioned
city_state_counts <- tidy_lyrics_city %>% 
  group_by(state_name) %>% 
  dplyr::summarise(n = sum(cnt)) %>%
  arrange(desc(n))
print(city_state_counts)
```


#### Join city counts to state counts and compute new rate
```{r city_state_join, comment=''}
state_city_counts_zeros <- left_join(state_counts_zeros,
                                     city_state_counts,
                                     by = "state_name") %>%
  dplyr::rename(n_state = n.x, n_city = n.y) %>%
  mutate(n_city = ifelse(is.na(n_city), 0, n_city),
         n_city_state = n_state + n_city,
         city_state_rate = (n_city_state / (pop2014 / 100000)) + zero_rate)

# same as above, but no states with zero mentioned by city or state
state_city_counts <- filter(state_city_counts_zeros, n_city_state > zero_rate)
```



## Fun facts...
### Some interesting things about these data
1. States mentioned by their cities, but not the state itself
```{r fun_facts, comment=''}
# Boston = most mentioned city without its state
all_the_cities <- filter(state_city_counts, !state_name %in% state_counts$state_name) %>%
  dplyr::select(name) %>%
  mutate_if(is.factor, as.character) %>%
  left_join(tidy_lyrics_city, by = c("name" = "state")) %>%
  dplyr::select(name, Song, Artist, city)

kable(all_the_cities)
```

2. What song mentions the most unique state names?
```{r fun_facts2, comment=''}
n_states_mentioned <- tidy_lyrics_state %>%
  group_by(Song) %>%
  dplyr::summarise(n = n()) %>%
  arrange(desc(n))
kable(n_states_mentioned)

filter(tidy_lyrics_state, Song == as.character(n_states_mentioned[1,1])) %>%
  dplyr::select(Song, Artist, Year, state_name)
```

3 What song mentions a single state the most number of times?
```{r fun_facts3, comment=''}
most_repeated_in_song <- right_join(tidy_lyrics, pop_df,
                                by = c("lyric" = "state_name")) %>%
  group_by(Song, Artist, lyric) %>%   
  dplyr::summarise(n = n()) %>%
  arrange(desc(n)) %>%
  ungroup() %>%
  filter(row_number() <= 10)

kable(most_repeated_in_song)
```



## Parameter estimation
### Estimate the parameters of the beta distribution in 3 ways:
1. Maximum-likelihood Estimation (MLE) with `fitdistr` package
2. Limited memory BFGS with the `rstan` package
3. Hamiltonian Monte Carlo (HMC) with `Stan` and `rstan` package

#### MLE with `fitdistr`
Here I use `fitdist` to compare the parameter estimates and Log likelihood of three different distributions, beta, exponential, and log-normal. This method is very fast, but not as accurate as the full Bayesian estimate. Here the beta has the best fit.  You could explore different distributions that make sense for the data, but since Silge used beta, I am also.
```{r fitdistr, comment=''}
## beta boot for comparison
beta_fit <- fitdist(state_counts_zeros$rate,"beta") # best logLik
summary(beta_fit)
exp_fit <- fitdist(state_counts_zeros$rate,"exp")
summary(exp_fit)
lnorm_fit <- fitdist(state_counts_zeros$rate,"lnorm")
summary(lnorm_fit)
```


#### Optimizing in `rstan`
This is the first introduction to a Stan model contained in the character string `opt_chr1`.  Typically the Stan model is saved in a separate file and called, but I kept it all in-house for this analysis.
```{r stan_optim, message=FALSE, warning=FALSE, comment='', include=TRUE, results="hide", cache = TRUE}
opt_chr1 <- "
data {
  int<lower=0> N;
  real x[N];
}
parameters {
  real<lower = 0> alpha0;
  real<lower = 0> beta0;
}
model {
  alpha0 ~ normal(0, 1);
  beta0 ~ normal(0, 10);
  //target += beta_lpdf(x | alpha0, beta0); // same as below
  x ~ beta(alpha0, beta0);
}
"
# initialize parameter values (based on knowledge or fitdist results)
init_list <- list(alpha0 = 0.1, beta0 = 1)
# compile model (~ 10 to 15 seconds)
opt_mod1 <- stan_model(model_code = opt_chr1)
# optimize data given model
opt1 <- optimizing(object = opt_mod1, as_vector = FALSE,
                   data = list(x = state_counts_zeros$rate,
                               N = length(state_counts_zeros$rate)),
                   hessian = TRUE,
                   draws = 2500)
```


#### Parameters estimates and log likelihood
```{r optim_results, comment=''}
# view results
opt1$par 
opt1$value #compare to LogLikelihood of summary(beta_fit)
```


plot distribution of parameters to see dispersal and correlation 
```{r param_plot, comment='', fig.align="center"}
ggplot(data.frame(opt1$theta_tilde), aes(x = alpha0, y = beta0)) +
  geom_density2d(aes(colour =..level..)) + 
  scale_colour_gradient(low="gray80",high="firebrick") + 
  geom_point(color = "skyblue3", alpha = 0.35) + 
  theme_bw() +
  theme(
    legend.position = "none"
  )
```



## Hamiltonian Monte Carlo (HMC)
#### Estimation and prediction
As the third method of estimation, we use a full Stan model and include a `generated quantities` block to make predictions of new rates for each state.  In this block, new parameters for each state are drawn and calculated into expected mention rates based on the observed number of mentions and population.  Estimating this within the model allows for the full integration over uncertainty.  Another approach is to take the `alpha` and `beta` parameters distributions, sample those outside the model and calculate the state estimates.

```{r, stan_fit, comment='', cache=TRUE}
model_string1_pred <- "
data {
  int<lower=1> N;
  vector[N] x;
  int<lower=1> M;
  vector[M] new_success;
  vector[M] new_attempts;
}
parameters {
  real<lower=0> alpha0;
  real<lower=0> beta0;
}
model {
  alpha0 ~ normal(0, 1);
  beta0 ~ normal(0, 10);
  x ~ beta(alpha0, beta0);
} generated quantities {
  vector[M] x_tilde; 
  for (n in 1:M)
    x_tilde[n] = beta_rng((new_success[n] + alpha0),
                (new_attempts[n] - new_success[n]  + beta0));
}
"
```


### Run the Stan model
```{r sta_fit_data, message=FALSE, warning=FALSE, comment='', include=TRUE, results="hide"}
new_success = state_counts_zeros$n
new_attempts = (state_counts_zeros$pop2014)/100000
model_dat1_pred <- list(x = state_counts_zeros$rate, 
                   N = length(state_counts_zeros$rate),
                   new_success = new_success,
                   new_attempts = new_attempts,
                   M = length(new_success))
fit1_pred <- stan(model_code = model_string1_pred, 
                  data = model_dat1_pred,
                  iter = 10000, chains = 4,  warmup=2500)
```


#### Extract results
Show the 95% Credible Interval for `alpha`, `beta`, each state, and the log posterior `lp__`
```{r print_dat1_model, comment=''}
fit1_pred_summary <- data.frame(summary(fit1_pred)[["summary"]]) %>%
  rownames_to_column() %>%
  mutate(Parameter = c("alpha0", "beta0",
                  as.character(state_counts_zeros$name), "lp__")) %>%
  dplyr::select(Parameter, mean, sd, X2.5., X97.5., n_eff, Rhat) %>%
  dplyr::rename(Mean = mean,
                SD = sd,
                `2.5%` = X2.5.,
                `97.5%` = X97.5.)
kable(fit1_pred_summary, digits = 3)
```


#### Realtionship of Bayesian to observed estimate
The plot design here is based entirely on Silge's visualization
```{r state_estimate, message=FALSE, warning=FALSE, comment='', fig.height=6, fig.width=6, fig.align="center"}
state_estimates <- rstan::extract(fit1_pred, pars = "x_tilde") %>%
  data.frame() %>%
  rename_(.dots=setNames(names(.),state_counts_zeros$state_name)) %>%
  gather() %>%
  dplyr::rename(state_name = key) %>%
  group_by(state_name) %>%
  dplyr::summarise(q025 = quantile(value, probs = 0.025),
                   q5 = quantile(value, probs = 0.5),
                   q975 = quantile(value, probs = 0.975),
                   mean = mean(value)) %>%
  left_join(.,state_counts_zeros)

### could melt and add q025,q5,q975 by color/shape
### could also predict across range of rates and show areas
ggplot(state_estimates, aes(rate, mean, color = n)) +
  geom_abline(intercept = 0, slope = 1, color = "gray70", linetype = 2) +
  # geom_text_repel(aes(rate, mean, label = state_name)) +
  geom_point(size = 4) +
  scale_color_gradient(low = "midnightblue", high = "pink",
                       name="Number\nof songs") +
  labs(title = "States in Song Lyrics with Empirical Bayes",
       subtitle = "States like Montana and Hawaii (high rates, few mentions) are shifted the most",
       x = "Measured rate of mentions per 100k population",
       y = "Empirical Bayes estimate of rate per 100k population") +
  theme_minimal(base_family = "Trebuchet MS") +
  theme(plot.title=element_text(family="Trebuchet MS"))
```


#### Bayesian vs. Observed estimate and 95% CI by state
The plot design here is based entirely on Silge's visualization
```{r state_estim_plot, comment='', fig.height=8, fig.width=6, fig.align="center"}
state_estimates %>% 
  arrange(desc(mean)) %>% 
  mutate(state_name = factor(name, levels = rev(unique(name)))) %>%
  dplyr::select(state_name, 'Measured rate' = rate, 
         'Bayesian estimate' = mean, q025, q975) %>% 
  gather(type, rate, `Measured rate`, `Bayesian estimate`) %>%
  ggplot(aes(rate, state_name, color = type)) +
  geom_errorbarh(aes(xmin = q025, xmax = q975), color = "gray50") +
  geom_point(size = 3) +
  xlim(0, NA) +
  labs(x = "Rate of mentions per million population",
       y = NULL, title = "Measured Rates, Bayesian Estimates (MCMC), and Credible Intervals",
       subtitle = "The 95% credible intervals are shown for these states") +
  theme_minimal(base_family = "Trebuchet MS") +
  theme(plot.title=element_text(family="Trebuchet MS", face = "bold")) +
  theme(legend.title=element_blank())
```


## Adding City Counts
### Below is a repeate of the analysis above, but using that dataset that includes the additional counts of cities mentions:  `state_city_counts_zeros`
```{r state_city_fit, comment=''}
new_success_SC = state_city_counts_zeros$n_city_state
new_attempts_SC = (state_city_counts_zeros$pop2014)/100000
model_SC_pred <- list(x = state_city_counts_zeros$city_state_rate, 
                   N = length(state_city_counts_zeros$city_state_rate),
                   new_success = new_success_SC,
                   new_attempts = new_attempts_SC,
                   M = length(new_success_SC))
```


#### Fit same Stan model as above, but new data.
```{r fit_SC_model, echo=TRUE, message=FALSE, warning=FALSE, cache=TRUE, comment='', results="hide"}
fit_SC_pred <- stan(model_code = model_string1_pred, 
                  data = model_SC_pred,
                  iter = 10000, chains = 4,  warmup=2500)
```


#### Summarise Stan fit
```{r print_model_SC_pred, comment=''}
fit_SC_pred_summary <- data.frame(summary(fit_SC_pred)[["summary"]]) %>%
  rownames_to_column() %>%
  mutate(Parameter = c("alpha0", "beta0",
                  as.character(state_counts_zeros$name), "lp__")) %>%
  dplyr::select(Parameter, mean, sd, X2.5., X97.5., n_eff, Rhat) %>%
  dplyr::rename(Mean = mean,
                SD = sd,
                `2.5%` = X2.5.,
                `97.5%` = X97.5.)
kable(fit_SC_pred_summary,  digits = 3)
```


#### Prepare fit estimates for plotting
```{r state_city_estimates, comment=''}
state_city_estimates <- rstan::extract(fit_SC_pred, pars = "x_tilde") %>%
  data.frame() %>%
  rename_(.dots=setNames(names(.),state_city_counts_zeros$state_name)) %>%
  gather() %>%
  dplyr::rename(state_name = key) %>%
  group_by(state_name) %>%
  dplyr::summarise(q025 = quantile(value, probs = 0.025),
                   q5 = quantile(value, probs = 0.5),
                   q975 = quantile(value, probs = 0.975),
                   mean = mean(value)) %>%
  left_join(.,state_city_counts_zeros)
```


#### Realtionship of Bayesian to observed state + city estimate
The plot design here is based entirely on Silge's visualization
```{r state_city_dot_plot, comment='', fig.height=6, fig.width=6, fig.align="center"}
### could melt and add q025,q5,q975 by color/shape
### could also predict across range of rates and show areas
ggplot(state_city_estimates, aes(city_state_rate, mean, color = n_city_state)) +
  geom_abline(intercept = 0, slope = 1, color = "gray70", linetype = 2) +
  # geom_text_repel(aes(rate, mean, label = state_name)) +
  geom_point(size = 4) +
  scale_color_gradient(low = "midnightblue", high = "pink",
                       name="Number\nof songs") +
  labs(title = "States in Song Lyrics with Empirical Bayes",
       subtitle = "States like Montana and Hawaii (high rates, few mentions) are shifted the most",
       x = "Measured rate of mentions per 100k population",
       y = "Empirical Bayes estimate of rate per 100k population") +
  theme_minimal(base_family = "Trebuchet MS") +
  theme(plot.title=element_text(family="Trebuchet MS"))
```


#### Bayesian vs. Observed state + city estimate and 95% CI by state
The plot design here is based entirely on Silge's visualization
```{r state_city_range_plot, comment='', fig.height=8, fig.width=6, fig.align="center"}
### range estiamtes plot
state_city_estimates %>% 
  arrange(desc(mean)) %>% 
  mutate(state_name = factor(name, levels = rev(unique(name)))) %>%
  dplyr::select(state_name, 'Measured rate' = city_state_rate, 
         'Bayesian estimate' = mean, q025, q975) %>% 
  gather(type, city_state_rate, `Measured rate`, `Bayesian estimate`) %>%
  ggplot(aes(city_state_rate, state_name, color = type)) +
  geom_errorbarh(aes(xmin = q025, xmax = q975), color = "gray50") +
  geom_point(size = 3) +
  xlim(0, NA) +
  labs(x = "Rate of mentions per 100k population",
       y = NULL, title = "Measured Rates, Bayesian Estimates (MCMC), and Credible Intervals",
       subtitle = "The 95% credible intervals are shown for these states") +
  theme_minimal(base_family = "Trebuchet MS") +
  theme(plot.title=element_text(family="Trebuchet MS", face = "bold")) +
  theme(legend.title=element_blank())
```



## Model Comparison
There are lots of ways to compare which model is "better", but I am not going to go crazy with it.  Log posteriors from the Stan models *can* be compared, but I do not believe they can be used for inference directly from the form they are reported from the fit.  We could also do things like hold-out samples, cross-validations, Leave-One-Out CV, or use information criteria such as WAIC.  [See Gelman](http://andrewgelman.com/2014/05/26/waic-cross-validation-stan/) and go down the rabbit hole from there. 

Here I take a simple approach and calculate a few metrics based a few things:
* Loss Metrics
    + Mean Absolute Error (MAE)
    + Root Mean Square Error (RMSE)
* Credible Interval (CI)
    + mean width of 95% CI
* Predicted Mentions
    + RMSE of expected mentions vs observed mentions
    + MAE of the same
    
These result show that the state counts only model *may* have the slightest advantage over the states + cities counts, but it really is very small.  For me, the choice of model would simply be which fits my purpose better; both are pretty darn good at estimating the number of mentions.  the MAE of expected mentions per state is less than 0.2 of a mention; rounded down to no real error at all in that category.  The beta distribution accurately described these data
```{r model_RMSE, comment=''}
city_state_error <- state_city_estimates %>%
  mutate(rate_error = mean - city_state_rate,
         pred_mentions = round(mean * (pop2014/100000),1),
         mention_error = n_city_state - pred_mentions,
         CI_width = q975 - q025) %>%
  dplyr::summarise(RMSE_rate = sqrt(mean(rate_error^2)),
            MAE_rate = mean(abs(rate_error)),
            mean_CI = mean(CI_width),
            RMSE_mentions = sqrt(mean(mention_error^2)),
            MAE_mentions = mean(abs(mention_error))) %>%
  as.numeric()

state_error <- state_estimates %>%
  mutate(rate_error = mean - rate,
         pred_mentions = round(mean * (pop2014/100000),1),
         mention_error = n - pred_mentions,
         CI_width = q975 - q025) %>%
  dplyr::summarise(RMSE_rate = sqrt(mean(rate_error^2)),
            MAE_rate = mean(abs(rate_error)),
            median_CI = median(CI_width),
            RMSE_mentions = sqrt(mean(mention_error^2)),
            MAE_mentions = mean(abs(mention_error))) %>%
  as.numeric()

#print
model_rmse <- data.frame(model = c("States Only", "City and States"),
           RMSE_rate =  c(state_error[1], city_state_error[1]),
           MAE_rate =  c(state_error[2], city_state_error[2]),
           Median_CI =  c(state_error[3], city_state_error[3]),
           RMSE_mentions =  c(state_error[4], city_state_error[4]),
           MAE_mentions =  c(state_error[5], city_state_error[5]))
kable(model_rmse, digits = 3)
```





### Thanks for reading!!!!